<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Implicit differentiation · Nonconvex.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Nonconvex.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Getting started</a></li><li><span class="tocitem">Problem definition</span><ul><li><a class="tocitem" href="../../problem/problem/">Overview</a></li><li><a class="tocitem" href="../../problem/model/"><code>Model</code> definition</a></li><li><a class="tocitem" href="../../problem/dict_model/"><code>DictModel</code> definition</a></li><li><a class="tocitem" href="../../problem/queries/">Querying models</a></li></ul></li><li><span class="tocitem">Gradients, Jacobians and Hessians</span><ul><li><a class="tocitem" href="../gradients/">Overview</a></li><li><a class="tocitem" href="../user_defined/">User-defined gradient, Jacobian or Hessian</a></li><li><a class="tocitem" href="../other_ad/">Using other AD packages</a></li><li><a class="tocitem" href="../chainrules_fd/">Using ChainRules in ForwardDiff</a></li><li><a class="tocitem" href="../sparse/">Sparse Jacobian or Hessian</a></li><li><a class="tocitem" href="../symbolic/">Symbolic differentiation</a></li><li class="is-active"><a class="tocitem" href>Implicit differentiation</a><ul class="internal"><li><a class="tocitem" href="#Background"><span>Background</span></a></li><li><a class="tocitem" href="#Relationship-to-[ImplicitDifferentiation.jl](https://github.com/gdalle/ImplicitDifferentiation.jl)"><span>Relationship to <code>ImplicitDifferentiation.jl</code></span></a></li><li><a class="tocitem" href="#Explicit-parameters"><span>Explicit parameters</span></a></li><li><a class="tocitem" href="#Implicit-parameters"><span>Implicit parameters</span></a></li><li><a class="tocitem" href="#Matrix-free-linear-solver-in-the-adjoint"><span>Matrix-free linear solver in the adjoint</span></a></li><li><a class="tocitem" href="#Arbitrary-data-structures"><span>Arbitrary data structures</span></a></li><li><a class="tocitem" href="#Tolerance"><span>Tolerance</span></a></li></ul></li><li><a class="tocitem" href="../history/">Storing history of gradients</a></li></ul></li><li><span class="tocitem">Algorithms</span><ul><li><a class="tocitem" href="../../algorithms/algorithms/">Overview</a></li><li><a class="tocitem" href="../../algorithms/mma/">Method of moving asymptotes in pure Julia</a></li><li><a class="tocitem" href="../../algorithms/ipopt/">Interior point method using <code>Ipopt.jl</code></a></li><li><a class="tocitem" href="../../algorithms/nlopt/">Various optimization algorithms from <code>NLopt.jl</code></a></li><li><a class="tocitem" href="../../algorithms/auglag/">Augmented Lagrangian algorithm in pure Julia</a></li><li><a class="tocitem" href="../../algorithms/minlp/">First and second order mixed integer nonlinear programming algorithms</a></li><li><a class="tocitem" href="../../algorithms/hyperopt/">Multi-start and hyper-parameter optimization in pure Julia</a></li><li><a class="tocitem" href="../../algorithms/surrogate/">Surrogate-assisted continuous and discrete, constrained optimization</a></li><li><a class="tocitem" href="../../algorithms/mts/">Multi-trajectory search algorithm in pure Julia</a></li><li><a class="tocitem" href="../../algorithms/sdp/">Interior point meta-algorithm for handling nonlinear semidefinite constraints</a></li><li><a class="tocitem" href="../../algorithms/metaheuristics/">A collection of meta-heuristic algorithms in pure Julia</a></li><li><a class="tocitem" href="../../algorithms/nomad/">Nonlinear optimization with the MADS (NOMAD) algorithm for continuous and discrete, constrained optimization</a></li><li><a class="tocitem" href="../../algorithms/tobs/">Topology optimization of binary structures (TOBS), a nonlinear binary optimization heuristic</a></li></ul></li><li><a class="tocitem" href="../../result/">Optimization result</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Gradients, Jacobians and Hessians</a></li><li class="is-active"><a href>Implicit differentiation</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Implicit differentiation</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaNonconvex/Nonconvex.jl/blob/master/docs/src/gradients/implicit.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Implicit-differentiation"><a class="docs-heading-anchor" href="#Implicit-differentiation">Implicit differentiation</a><a id="Implicit-differentiation-1"></a><a class="docs-heading-anchor-permalink" href="#Implicit-differentiation" title="Permalink"></a></h1><h2 id="Background"><a class="docs-heading-anchor" href="#Background">Background</a><a id="Background-1"></a><a class="docs-heading-anchor-permalink" href="#Background" title="Permalink"></a></h2><p>Differentiating implicit functions efficiently using the implicit function theorem has many applications including:</p><ul><li>Nonlinear partial differential equation constrained optimization</li><li>Differentiable optimization layers in deep learning (aka deep declarative networks)</li><li>Differentiable fixed point iteration algorithms for optimal transport (e.g. the Sinkhorn methods)</li><li>Gradient-based bi-level and robust optimization (aka anti-optimization)</li><li>Multi-parameteric programming (aka optimization sensitivity analysis)</li></ul><p>For more on implicit differentation, refer to the last part of the <a href="https://www.youtube.com/watch?v=UqymrMG-Qi4"><em>Understanding automatic differentiation (in Julia)</em></a> video on YouTube and the <a href="https://arxiv.org/abs/2105.15183"><em>Efficient and modular implicit differentiation</em></a> manuscript for an introduction to the methods implemented here.</p><h2 id="Relationship-to-[ImplicitDifferentiation.jl](https://github.com/gdalle/ImplicitDifferentiation.jl)"><a class="docs-heading-anchor" href="#Relationship-to-[ImplicitDifferentiation.jl](https://github.com/gdalle/ImplicitDifferentiation.jl)">Relationship to <a href="https://github.com/gdalle/ImplicitDifferentiation.jl"><code>ImplicitDifferentiation.jl</code></a></a><a id="Relationship-to-[ImplicitDifferentiation.jl](https://github.com/gdalle/ImplicitDifferentiation.jl)-1"></a><a class="docs-heading-anchor-permalink" href="#Relationship-to-[ImplicitDifferentiation.jl](https://github.com/gdalle/ImplicitDifferentiation.jl)" title="Permalink"></a></h2><p><a href="https://github.com/gdalle/ImplicitDifferentiation.jl"><code>ImplicitDifferentiation.jl</code></a> is an attempt to simplify the implementation in <code>Nonconvex</code> making it more lightweight and better documented. For instance, the <a href="https://gdalle.github.io/ImplicitDifferentiation.jl/">documentation of <code>ImplicitDifferentiation</code></a> presents a number of examples of implicit functions all of which can be defined using <code>Nonconvex</code> instead.</p><h2 id="Explicit-parameters"><a class="docs-heading-anchor" href="#Explicit-parameters">Explicit parameters</a><a id="Explicit-parameters-1"></a><a class="docs-heading-anchor-permalink" href="#Explicit-parameters" title="Permalink"></a></h2><p>There are 4 components to any implicit function:</p><ol><li>The parameters <code>p</code></li><li>The variables <code>x</code></li><li>The residual <code>f(p, x)</code> which is used to define <code>x(p)</code> as the <code>x</code> which satisfies <code>f(p, x) == 0</code> for a given value <code>p</code></li><li>The algorithm used to evaluate <code>x(p)</code> satisfying the condition <code>f(p, x) == 0</code></li></ol><p>In order to define a differentiable implicit function using <code>Nonconvex</code>, you have to specify the &quot;forward&quot; algorithm which finds <code>x(p)</code>. For instance, consider the following example:</p><pre><code class="language-julia hljs">using SparseArrays, NLsolve, Zygote, Nonconvex

N = 10
A = spdiagm(0 =&gt; fill(10.0, N), 1 =&gt; fill(-1.0, N-1), -1 =&gt; fill(-1.0, N-1))
p0 = randn(N)

f(p, x) = A * x + 0.1 * x.^2 - p
function forward(p)
  # Solving nonlinear system of equations
  sol = nlsolve(x -&gt; f(p, x), zeros(N), method = :anderson, m = 10)
  # Return the zero found (ignore the second returned value for now)
  return sol.zero, nothing
end</code></pre><p><code>forward</code> above solves for <code>x</code> in the nonlinear system of equations <code>f(p, x) == 0</code> given the value of <code>p</code>. In this case, the residual function is the same as the function <code>f(p, x)</code> used in the forward pass. One can then use the 2 functions <code>forward</code> and <code>f</code> to define an implicit function using:</p><pre><code class="language-julia hljs">imf = ImplicitFunction(forward, f)
xstar = imf(p0)</code></pre><p>where <code>imf(p0)</code> solves the nonlinear system for <code>p = p0</code> and returns the zero <code>xstar</code> of the nonlinear system. This function can now be part of any arbitrary Julia function differentiated by Zygote, e.g. it can be part of an objective function in an optimization problem using gradient-based optimization:</p><pre><code class="language-julia hljs">obj(p) = sum(imf(p))
g = Zygote.gradient(obj, p0)[1]</code></pre><p>In the implicit function&#39;s adjoint rule definition, the partial Jacobian <code>∂f/∂x</code> is used according to the implicit function theorem. Often this Jacobian or a good approximation of it might be a by-product of the <code>forward</code> function. For example when the <code>forward</code> function does an optimization using a BFGS-based approximation of the Hessian of the Lagrangian function, the final BFGS approximation can be a good approximation of <code>∂f/∂x</code> where the residual <code>f</code> is the gradient of the Lagrangian function wrt <code>x</code>. In those cases, this Jacobian by-product can be returned as the second argument from <code>forward</code> instead of <code>nothing</code>.</p><h2 id="Implicit-parameters"><a class="docs-heading-anchor" href="#Implicit-parameters">Implicit parameters</a><a id="Implicit-parameters-1"></a><a class="docs-heading-anchor-permalink" href="#Implicit-parameters" title="Permalink"></a></h2><p>In some cases, it may be more convenient to avoid having to specify <code>p</code> as an explicit argument in <code>forward</code> and <code>f</code>. The following is also valid to use and will give correct gradients with respect to <code>p</code>:</p><pre><code class="language-julia hljs">function obj(p)
  N = length(p)
  f(x) = A * x + 0.1 * x.^2 - p
  function forward()
    # Solving nonlinear system of equations
    sol = nlsolve(f, zeros(N), method = :anderson, m = 10)
    # Return the zero found (ignore the second returned value for now)
    return sol.zero, nothing
  end
  imf = ImplicitFunction(forward, f)
  return sum(imf())
end
g = Zygote.gradient(obj, p0)[1]</code></pre><p>Notice that <code>p</code> was not an explicit argument to <code>f</code> or <code>forward</code> in the above example and that the implicit function is called using <code>imf()</code>. Using some explicit parameters and some implicit parameters is also supported.</p><h2 id="Matrix-free-linear-solver-in-the-adjoint"><a class="docs-heading-anchor" href="#Matrix-free-linear-solver-in-the-adjoint">Matrix-free linear solver in the adjoint</a><a id="Matrix-free-linear-solver-in-the-adjoint-1"></a><a class="docs-heading-anchor-permalink" href="#Matrix-free-linear-solver-in-the-adjoint" title="Permalink"></a></h2><p>In the adjoint definition of implicit functions, a linear system:</p><pre><code class="language-julia hljs">(df/dy) * x = v</code></pre><p>is solved to find the adjoint vector. To solve the system using a matrix-free iterative solver (GMRES by default) that avoids constructing the Jacobian <code>df/dy</code>, you can set the <code>matrixfree</code> keyword argument to <code>true</code> (default is <code>false</code>). When set to <code>false</code>, the entrie Jacobian matrix is formed and the linear system is solved using LU factorization.</p><h2 id="Arbitrary-data-structures"><a class="docs-heading-anchor" href="#Arbitrary-data-structures">Arbitrary data structures</a><a id="Arbitrary-data-structures-1"></a><a class="docs-heading-anchor-permalink" href="#Arbitrary-data-structures" title="Permalink"></a></h2><p>Both <code>p</code> and <code>x</code> above can be arbitrary data structures, not just arrays of numbers.</p><h2 id="Tolerance"><a class="docs-heading-anchor" href="#Tolerance">Tolerance</a><a id="Tolerance-1"></a><a class="docs-heading-anchor-permalink" href="#Tolerance" title="Permalink"></a></h2><p>The implicit function theorem assumes that some conditions <code>f(p, x) == 0</code> is satisfied. In practice, this will only be approximately satisfied. When this condition is violated, the gradient reported by the implicit function theorem cannot be trusted since its assumption is violated. The maximum tolerance allowed to &quot;accept&quot; the solution <code>x(p)</code> and the gradient is given by the keyword argument <code>tol</code> (default value is <code>1e-5</code>). When the norm of the residual function <code>f(p, x)</code> is greater than this tolerance, <code>NaN</code>s  are returned for the gradient instead of the value computed via the implicit function theorem. If additionally, the keyword argument <code>error_on_tol_violation</code> is set to <code>true</code> (default value is <code>false</code>), an error is thrown if the norm of the residual exceeds the specified tolerance <code>tol</code>.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../symbolic/">« Symbolic differentiation</a><a class="docs-footer-nextpage" href="../history/">Storing history of gradients »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.22 on <span class="colophon-date" title="Monday 25 July 2022 13:39">Monday 25 July 2022</span>. Using Julia version 1.7.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
